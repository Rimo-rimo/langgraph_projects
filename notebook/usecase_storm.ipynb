{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Optional, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "long_context_llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline 생성을 위한 작업\n",
    "* 사용자의 topic에 대해서, wikipedia 형식의 초안을 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n",
    "        ),\n",
    "        (\"user\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
    "    \n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and descriptions for each section of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "\n",
    "generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "An overview of language models and their relevance in natural language processing (NLP). Introduction to the concept of a context window, particularly in relation to million-plus token models and their significance in Retrieval-Augmented Generation (RAG).\n",
      "\n",
      "## Understanding Language Models\n",
      "\n",
      "A detailed explanation of language models, including their architecture, training processes, and the evolution from traditional models to modern transformer-based models.\n",
      "\n",
      "## Context Windows in Language Models\n",
      "\n",
      "Definition and importance of context windows in language models. Discussion on how larger context windows enhance model capabilities in understanding and generating human-like text.\n",
      "\n",
      "## Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "An explanation of the RAG framework, including its components: retrieval and generation. Discussion on how RAG improves the performance of language models by incorporating external knowledge.\n",
      "\n",
      "## Impact of Million-Plus Token Context Windows on RAG\n",
      "\n",
      "Analysis of how million-plus token context windows influence the effectiveness and efficiency of RAG models. Key points include improved coherence, relevance of generated text, and memory handling.\n",
      "\n",
      "## Benefits of Large Context Windows in RAG\n",
      "\n",
      "Exploration of the advantages brought by large context windows, such as enhanced contextual understanding, better handling of long documents, and the ability to maintain conversational context over extended interactions.\n",
      "\n",
      "## Challenges and Limitations\n",
      "\n",
      "A discussion of the challenges faced when implementing million-plus token context window models in RAG, including computational costs, data retrieval efficiency, and potential biases.\n",
      "\n",
      "## Case Studies and Applications\n",
      "\n",
      "Examples of successful implementations of million-plus token context window models in RAG applications across various fields such as healthcare, customer support, and content generation.\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "Speculation on the future development of language models and RAG frameworks. Consideration of potential advancements in context window sizes, model architectures, and applications.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "A recap of the key points discussed in the article, emphasizing the transformative potential of million-plus token context windows in enhancing RAG systems.\n",
      "\n",
      "## References\n",
      "\n",
      "A list of scholarly articles, books, and other resources referenced throughout the article.\n"
     ]
    }
   ],
   "source": [
    "example_topic = \"Impact of million-plus token context window language models on RAG\"\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "\n",
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Topics\n",
    "* 검색엔진을 활용해 관련성있는 최신 정보를 통합 하여 더 나은 결과로 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.\n",
    "       Please list the as many subjects and urls as you can.\n",
    "       Topic of interest: {topic}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"Comprehensive list of related subjects as background research.\",\n",
    "    )\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(\n",
    "    RelatedSubjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['Impact of million-plus token context window language models on RAG', 'Language models', 'Contextual language models', 'Retrieval-augmented generation (RAG)', 'Natural language processing', 'Machine learning', 'Artificial intelligence', 'Transformers in AI', 'Text generation', 'Large language models', 'Tokenization in NLP', 'Applications of language models', 'AI and language understanding', 'Ethics in AI'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor.\", pattern=r'^[a-zA-Z0-9_-]+$'\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"Comprehensive list of editors with their roles and affiliations. The name should be this pattern : ^[a-zA-Z0-9_-]+$\",\n",
    "        # Add a pydantic validation/restriction to be at most M editors\n",
    "    )\n",
    "\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic.\\\n",
    "    You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.\n",
    "\n",
    "    Wiki page outlines of related topics for inspiration:\n",
    "    {examples}\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Topic of interest: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o\"\n",
    ").with_structured_output(Perspectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata[\"categories\"])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[\n",
    "        :max_length\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str):\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'AI Research Institute',\n",
       "   'name': 'Dr_Samantha_Lee',\n",
       "   'role': 'Lead Researcher',\n",
       "   'description': 'Dr. Lee will focus on the technical advancements of language models with million-plus token context windows and how these improvements enhance retrieval-augmented generation (RAG) capabilities. She will explore the scalability of these models and their potential to process and generate more accurate and contextually rich responses.'},\n",
       "  {'affiliation': 'OpenAI',\n",
       "   'name': 'John_Martinez',\n",
       "   'role': 'AI Ethicist',\n",
       "   'description': 'John will explore the ethical implications of using large context window language models in RAG, including issues related to data privacy, bias, and misinformation. He aims to ensure that these models are developed and deployed responsibly.'},\n",
       "  {'affiliation': 'Tech Industry Consultant',\n",
       "   'name': 'Emily_Tan',\n",
       "   'role': 'Industry Analyst',\n",
       "   'description': 'Emily will analyze the commercial applications and market impact of integrating million-plus token context window language models into RAG systems. Her focus will be on identifying key industries that benefit from these advancements and predicting future trends.'},\n",
       "  {'affiliation': 'University of Cambridge',\n",
       "   'name': 'Prof_Alan_Smith',\n",
       "   'role': 'Academic Scholar',\n",
       "   'description': 'Prof. Smith will provide a historical perspective on the evolution of language models, from their inception to the development of million-plus token context windows. He will discuss the theoretical underpinnings and breakthroughs that made this advancement possible.'},\n",
       "  {'affiliation': 'Data Privacy Advocacy Group',\n",
       "   'name': 'Nina_Roberts',\n",
       "   'role': 'Privacy Advocate',\n",
       "   'description': 'Nina will address the data privacy concerns associated with using extensive language models in RAG systems. She will advocate for transparent data usage policies and the importance of safeguarding user data against unauthorized access or misuse.'},\n",
       "  {'affiliation': 'AI Development Firm',\n",
       "   'name': 'Rajesh_Kumar',\n",
       "   'role': 'Software Engineer',\n",
       "   'description': 'Rajesh will focus on the practical implementation of million-plus token context window models in RAG systems. He will discuss the engineering challenges and solutions, including computational resource management and system optimization for real-time applications.'},\n",
       "  {'affiliation': 'Journalist and Tech Blogger',\n",
       "   'name': 'Lisa_Chen',\n",
       "   'role': 'Public Communicator',\n",
       "   'description': 'Lisa will translate complex technical information about these advanced language models and RAG systems into accessible content for the general public. Her goal is to raise awareness and understanding of the implications of these technologies in everyday life.'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interview state 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Dialog Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str):\n",
    "    converted = []\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    return {\"messages\": converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state[\"editor\"]\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, I'm focusing on how language models with million-plus token context windows can enhance Retrieval-Augmented Generation (RAG) systems. Can you explain how these extended context windows change the way RAG systems retrieve and generate information?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f\"So you said you were writing an article on {example_topic}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[-1],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "question[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Yes, I'm focusing on how language models with million-plus token context windows can enhance Retrieval-Augmented Generation (RAG) systems. Can you explain how these extended context windows change the way RAG systems retrieve and generate information?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 225, 'total_tokens': 270, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, name='Lisa_Chen', id='run-7192aff5-f39c-4ffc-b52e-de40ed4f21df-0', usage_metadata={'input_tokens': 225, 'output_tokens': 45, 'total_tokens': 270, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questions.\",\n",
    "    )\n",
    "\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o\"\n",
    ").with_structured_output(Queries, include_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['impact of large token context windows on RAG systems',\n",
       " 'extended context windows in language models and RAG systems',\n",
       " 'large context windows in language models and retrieval-augmented generation',\n",
       " 'how do million-plus token contexts enhance RAG systems',\n",
       " 'effects of large context windows on information retrieval and generation in RAG systems']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "queries[\"parsed\"].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\",\n",
    "    )\n",
    "    cited_urls: List[str] = Field(\n",
    "        description=\"List of urls cited in the answer.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
    "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "\n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
    "\n",
    "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ").with_config(run_name=\"GenerateAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "'''\n",
    "# Tavily is typically a better search engine, but your free queries are limited\n",
    "search_engine = TavilySearchResults(max_results=4)\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = tavily_search.invoke(query)\n",
    "    return [{\"content\": r[\"content\"], \"url\": r[\"url\"]} for r in results]\n",
    "'''\n",
    "\n",
    "# DDG\n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    "    name: str = \"Subject_Matter_Expert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries[\"parsed\"].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries[\"raw\"]\n",
    "    tool_call = queries[\"raw\"].tool_calls[0]\n",
    "    tool_id = tool_call[\"id\"]\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language models with million-plus token context windows significantly enhance Retrieval-Augmented Generation (RAG) systems by allowing them to ingest and process vast amounts of information simultaneously. This increased context capacity enables RAG systems to retrieve and utilize more extensive datasets, improving the relevance and accuracy of generated responses. For instance, traditional RAG systems often rely on retrieving a limited number of relevant documents or chunks of text; however, with extended context windows, the model can directly analyze these documents in their entirety, leading to a more nuanced understanding of the information at hand[^1^].\\n\\nMoreover, the integration of long context windows allows RAG systems to reduce the frequency of retrieval operations, as the model can hold more information at once, thereby decreasing latency and improving efficiency during the generation phase. This is particularly beneficial in scenarios where the data is complex or interrelated, as it allows the language model to maintain coherence and context throughout its responses[^2^].\\n\\nFurthermore, the use of large context windows can mitigate issues related to hallucination, where models generate incorrect or fabricated information due to insufficient context. By providing more comprehensive information, these models can ground their outputs in reality, leading to more reliable and fact-based responses[^3^]. \\n\\nLastly, while large context models offer substantial advantages, RAG systems remain crucial since they efficiently retrieve only the most relevant information, allowing for a dynamic and adaptable approach to information synthesis. This combination of extensive context and targeted retrieval enhances the overall performance of RAG systems, making them more effective in generating accurate and contextually rich responses[^4^].\\n\\nCitations:\\n\\n[1]: https://www.deepset.ai/blog/long-context-llms-rag\\n[2]: https://arxiv.org/abs/2407.16833\\n[3]: https://www.thecloudgirl.dev/blog/rag-vs-large-context-window\\n[4]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Construct the Interview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "from langgraph.pregel import RetryPolicy\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject_Matter_Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question, retry=RetryPolicy(max_attempts=5))\n",
    "builder.add_node(\"answer_question\", gen_answer, retry=RetryPolicy(max_attempts=5))\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.add_edge(START, \"ask_question\")\n",
    "interview_graph = builder.compile(checkpointer=False).with_config(\n",
    "    run_name=\"Conduct Interviews\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_question\n",
      "--  [AIMessage(content=\"Yes, that's correct! I'm particularly interested in how the advancements in language models with million-plus token context windows can improve retrieval-augmented generation (RAG) capabilities. Could you explain how these longer context windows contribute to the effectiveness of RAG systems in real-world applications?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 248, 'total_tokens': 303, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, name='Dr_Samantha_Lee', id='run-0cafffc3-a003-4da3-8590-5bc8eb913d36-0', usage_metadata={'input_tokens': 248, 'output_tokens': 55, 'total_tokens': 303, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n",
      "answer_question\n",
      "--  [AIMessage(content='The advancements in language models with million-plus token context windows significantly enhance retrieval-augmented generation (RAG) capabilities, which is beneficial for real-world applications. First, larger context windows enable models to process more information at once, allowing them to maintain context over longer documents or conversations. This leads to improved contextual understanding, which is crucial for generating accurate and relevant responses in applications such as customer support and academic research.\\n\\nMoreover, with the ability to access and utilize more extensive data within a single context, these models reduce the need for extensive preprocessing, such as chunking and indexing, which can be time-consuming and complex in traditional RAG setups. In systems with smaller context windows, significant effort is often required to extract relevant segments of text, which can potentially lead to loss of information and context. In contrast, larger context models can integrate and synthesize information more effectively, thus enhancing the generation of coherent responses without losing critical data points【5†source】【6†source】【8†source】.\\n\\nFurthermore, the integration of long-context language models can mitigate some challenges faced by traditional RAG systems, such as the need for external retrieval mechanisms. While RAG systems excel in customizability and efficiency by leveraging external sources, they may still struggle with factual inaccuracies when relying on retrieved data from unstructured sources. Long-context models, on the other hand, can directly process and generate responses based on the entirety of the input, thereby reducing reliance on potentially flawed external information【7†source】【10†source】.\\n\\nIn real-world applications, industries such as finance, healthcare, and technology can greatly benefit from the capabilities of these advanced models. For instance, in finance, the ability to analyze lengthy reports or multiple documents simultaneously leads to quicker insights and decision-making【6†source】【8†source】. Similarly, in healthcare, being able to reference extensive patient histories or medical literature in a single query enables healthcare professionals to generate more accurate treatment recommendations【9†source】【10†source】. Overall, the combination of larger context windows and RAG techniques opens up new possibilities for enhanced language understanding and generation across various sectors.\\n\\nCitations:\\n\\n[1]: https://www.sciencetimes.com/articles/51540/20241101/pushing-the-boundaries-of-contextual-understanding.htm\\n[2]: https://www.deepset.ai/blog/long-context-llms-rag\\n[3]: https://www.superannotate.com/blog/rag-vs-long-context-llms\\n[4]: https://www.databricks.com/blog/long-context-rag-performance-llms\\n[5]: https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/', additional_kwargs={}, response_metadata={}, name='Subject_Matter_Expert')]\n",
      "ask_question\n",
      "--  [AIMessage(content='Thank you for that detailed explanation! Could you elaborate on any specific challenges or limitations that still exist with million-plus token context models in the context of RAG? How do these challenges impact their deployment in practical applications?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 867, 'total_tokens': 910, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, name='Dr_Samantha_Lee', id='run-e6e79197-c5e1-490b-87c3-03680ee77cad-0', usage_metadata={'input_tokens': 867, 'output_tokens': 43, 'total_tokens': 910, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n",
      "answer_question\n",
      "--  [AIMessage(content='Despite the significant advancements offered by million-plus token context models, several challenges and limitations remain when these models are implemented in retrieval-augmented generation (RAG) systems. One of the primary challenges is the increased computational cost and resource requirements. Handling large context windows significantly escalates the demand for memory and processing power, which can hinder deployment in environments with limited resources. For instance, operating models capable of processing millions of tokens requires high-performance hardware, making them less accessible for smaller organizations or real-time applications that depend on rapid response times【6†source】【9†source】.\\n\\nAdditionally, the complexity of training and fine-tuning these models poses another hurdle. Training long-context models can be technically challenging and time-consuming, requiring sophisticated optimization techniques to ensure efficiency and stability. As the context length increases, the training process must account for a broader range of dependencies, which complicates the learning dynamics【5†source】【7†source】.\\n\\nAnother limitation is the potential for diminishing returns in performance as context windows grow larger. While longer context windows provide more information, they do not automatically translate to better performance in all tasks. In some instances, models may struggle with information overload, leading to confusion or inaccuracies in generated outputs. Therefore, simply increasing context size does not guarantee improved outcomes, particularly for tasks that require more nuanced understanding or reasoning【6†source】【8†source】.\\n\\nMoreover, the integration of long-context models with existing RAG frameworks can be challenging. RAG systems typically rely on external retrieval mechanisms to fetch relevant information, and the synergy between these models and traditional retrieval methods is still an area of research. Effective integration is crucial to ensure that the strengths of both approaches are utilized efficiently without introducing inconsistencies or errors in the response generation process【5†source】【7†source】.\\n\\nIn practical applications, these challenges can impact deployment strategies, as organizations must weigh the benefits of advanced models against their costs and complexities. In healthcare, for example, while the ability to analyze extensive patient histories is advantageous, the associated costs of deploying such systems can limit their application to larger healthcare providers【9†source】【10†source】. Similarly, in customer service, the computational demands of long-context models could impede their use in real-time chatbot applications, where rapid response times are essential【8†source】【9†source】. Overall, while million-plus token context models hold great promise, their practical deployment requires careful consideration of these ongoing challenges.\\n\\nCitations:\\n\\n[1]: https://www.sciencetimes.com/articles/51540/20241101/pushing-the-boundaries-of-contextual-understanding.htm\\n[2]: https://www.deepset.ai/blog/long-context-llms-rag\\n[3]: https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/\\n[4]: https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\\n[5]: https://venturebeat.com/ai/how-gradient-created-an-open-llm-with-a-million-token-context-window/\\n[6]: https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e', additional_kwargs={}, response_metadata={}, name='Subject_Matter_Expert')]\n",
      "ask_question\n",
      "--  [AIMessage(content=\"Thank you for the insightful information! Can you provide examples of specific industries or use cases where the integration of million-plus token context models and RAG has shown promising results, despite the challenges you've mentioned? What outcomes have been observed in these scenarios?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 1585, 'total_tokens': 1634, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, name='Dr_Samantha_Lee', id='run-1b4921b0-c971-40b9-87de-e9eceeef383c-0', usage_metadata={'input_tokens': 1585, 'output_tokens': 49, 'total_tokens': 1634, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n",
      "answer_question\n",
      "--  [AIMessage(content='Several industries have successfully integrated million-plus token context models with retrieval-augmented generation (RAG), showcasing promising results despite existing challenges. One notable example is the legal sector, where long-context models enable the analysis of extensive legal documents and case histories. By processing large volumes of text simultaneously, these models can generate precise legal insights and recommendations, streamlining research and case preparation. The outcome observed includes faster turnaround times for legal analyses and reduced reliance on manual review processes, enhancing overall efficiency【6†source】【9†source】.\\n\\nIn the realm of software development, million-plus token context models have been employed to improve code generation and debugging processes. These models facilitate the understanding of extensive codebases by referencing multiple files and documentation concurrently. As a result, developers have reported increased accuracy in code suggestions and a notable reduction in debugging time. This integration ultimately leads to enhanced productivity and innovation within development teams【5†source】【8†source】.\\n\\nThe healthcare industry also benefits from these models, particularly in generating treatment plans based on comprehensive patient histories and medical literature. By analyzing lengthy medical records and guidelines, long-context models can provide personalized recommendations and potential treatment pathways. The outcomes include improved patient outcomes and more informed decision-making by healthcare providers, as they can access a broader context of information【8†source】【9†source】.\\n\\nAdditionally, the financial sector has seen advantages through the use of long-context models for market analysis and risk assessment. These models can process vast datasets, including historical market trends and economic reports, generating insights that help financial analysts make quicker and more informed predictions. The outcomes noted include enhanced accuracy in financial forecasting and the ability to react promptly to market changes【7†source】【10†source】.\\n\\nIn summary, the integration of million-plus token context models with RAG has shown effective results across various industries, leading to improved efficiency, accuracy, and overall outcomes despite the challenges associated with their deployment.\\n\\nCitations:\\n\\n[1]: https://www.sciencetimes.com/articles/51540/20241101/pushing-the-boundaries-of-contextual-understanding.htm\\n[2]: https://contextual.ai/introducing-rag2/\\n[3]: https://www.databricks.com/blog/long-context-rag-capabilities-openai-o1-and-google-gemini\\n[4]: https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/\\n[5]: https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76', additional_kwargs={}, response_metadata={}, name='Subject_Matter_Expert')]\n",
      "ask_question\n",
      "--  [AIMessage(content='Thank you so much for your help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 2170, 'total_tokens': 2178, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1408}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, name='Dr_Samantha_Lee', id='run-78b8d98e-98f5-4647-905d-fccb7e79840a-0', usage_metadata={'input_tokens': 2170, 'output_tokens': 8, 'total_tokens': 2178, 'input_token_details': {'cache_read': 1408}, 'output_token_details': {'reasoning': 0}})]\n",
      "answer_question\n",
      "--  [AIMessage(content='Several industries have successfully integrated million-plus token context models with retrieval-augmented generation (RAG), showcasing promising results despite existing challenges. One notable example is the legal sector, where long-context models enable the analysis of extensive legal documents and case histories. By processing large volumes of text simultaneously, these models can generate precise legal insights and recommendations, streamlining research and case preparation. The outcome observed includes faster turnaround times for legal analyses and reduced reliance on manual review processes, enhancing overall efficiency.\\n\\nIn the realm of software development, million-plus token context models have been employed to improve code generation and debugging processes. These models facilitate the understanding of extensive codebases by referencing multiple files and documentation concurrently. As a result, developers have reported increased accuracy in code suggestions and a notable reduction in debugging time. This integration ultimately leads to enhanced productivity and innovation within development teams.\\n\\nThe healthcare industry also benefits from these models, particularly in generating treatment plans based on comprehensive patient histories and medical literature. By analyzing lengthy medical records and guidelines, long-context models can provide personalized recommendations and potential treatment pathways. The outcomes include improved patient outcomes and more informed decision-making by healthcare providers, as they can access a broader context of information.\\n\\nAdditionally, the financial sector has seen advantages through the use of long-context models for market analysis and risk assessment. These models can process vast datasets, including historical market trends and economic reports, generating insights that help financial analysts make quicker and more informed predictions. The outcomes noted include enhanced accuracy in financial forecasting and the ability to react promptly to market changes.\\n\\nIn summary, the integration of million-plus token context models with RAG has shown effective results across various industries, leading to improved efficiency, accuracy, and overall outcomes despite the challenges associated with their deployment.\\n\\nCitations:\\n\\n[1]: https://www.sciencetimes.com/articles/51540/20241101/pushing-the-boundaries-of-contextual-understanding.htm\\n[2]: https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\\n[3]: https://venturebeat.com/ai/how-gradient-created-an-open-llm-with-a-million-token-context-window/\\n[4]: https://www.deepset.ai/blog/long-context-llms-rag\\n[5]: https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/', additional_kwargs={}, response_metadata={}, name='Subject_Matter_Expert')]\n"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"Subject_Matter_Expert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name][\"messages\"]))\n",
    "final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
